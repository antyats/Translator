{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 7750385,
          "sourceType": "datasetVersion",
          "datasetId": 4531226
        }
      ],
      "dockerImageVersionId": 30665,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from typing import Iterable, List\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from timeit import default_timer as timer\n",
        "from torch.nn import Transformer\n",
        "from torch import Tensor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm.auto import tqdm\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import math\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-03-03T17:37:15.431770Z",
          "iopub.execute_input": "2024-03-03T17:37:15.432832Z",
          "iopub.status.idle": "2024-03-03T17:37:24.872194Z",
          "shell.execute_reply.started": "2024-03-03T17:37:15.432764Z",
          "shell.execute_reply": "2024-03-03T17:37:24.871327Z"
        },
        "trusted": true,
        "id": "TItrkyN9lMll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SRC_LANGUAGE = 'de'\n",
        "TGT_LANGUAGE = 'en'"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T17:37:24.873838Z",
          "iopub.execute_input": "2024-03-03T17:37:24.874321Z",
          "iopub.status.idle": "2024-03-03T17:37:24.878707Z",
          "shell.execute_reply.started": "2024-03-03T17:37:24.874292Z",
          "shell.execute_reply": "2024-03-03T17:37:24.877781Z"
        },
        "trusted": true,
        "id": "AkoQGrdVlMln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_transform = {}\n",
        "vocab_transform = {}\n",
        "token_transform[SRC_LANGUAGE] = get_tokenizer(None, language='de_core_web_sm')\n",
        "token_transform[TGT_LANGUAGE] = get_tokenizer(None, language='en_core_news_sm')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T17:37:24.879938Z",
          "iopub.execute_input": "2024-03-03T17:37:24.880211Z",
          "iopub.status.idle": "2024-03-03T17:37:24.893936Z",
          "shell.execute_reply.started": "2024-03-03T17:37:24.880187Z",
          "shell.execute_reply": "2024-03-03T17:37:24.893184Z"
        },
        "trusted": true,
        "id": "ZMe_GWmblMln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, train_file: str, target_file: str):\n",
        "        self.train_file = train_file\n",
        "        self.target_file = target_file\n",
        "\n",
        "        train_texts = []\n",
        "        with open(train_file, encoding=\"utf-8\") as file:\n",
        "            train_texts.extend(file.readlines())\n",
        "\n",
        "\n",
        "        self.train_texts = train_texts\n",
        "\n",
        "        target_texts = []\n",
        "        with open(target_file, encoding=\"utf-8\") as file:\n",
        "            target_texts.extend(file.readlines())\n",
        "\n",
        "\n",
        "        self.target_texts = target_texts\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.train_texts)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return (self.train_texts[item], self.target_texts[item])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T17:37:24.896012Z",
          "iopub.execute_input": "2024-03-03T17:37:24.896435Z",
          "iopub.status.idle": "2024-03-03T17:37:24.909052Z",
          "shell.execute_reply.started": "2024-03-03T17:37:24.896408Z",
          "shell.execute_reply": "2024-03-03T17:37:24.908296Z"
        },
        "trusted": true,
        "id": "ifwhWUhhlMln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TranslationDataset(\"/kaggle/input/bdz2aa/train.de-en.de\", \"/kaggle/input/bdz2aa/train.de-en.en\")\n",
        "valid_dataset = TranslationDataset(\"/kaggle/input/bdz2aa/val.de-en.de\", \"/kaggle/input/bdz2aa/val.de-en.en\")\n",
        "\n",
        "iterator = iter(train_dataset)\n",
        "print(next(iterator))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T17:37:24.910282Z",
          "iopub.execute_input": "2024-03-03T17:37:24.910994Z",
          "iopub.status.idle": "2024-03-03T17:37:25.578646Z",
          "shell.execute_reply.started": "2024-03-03T17:37:24.910960Z",
          "shell.execute_reply": "2024-03-03T17:37:25.577607Z"
        },
        "trusted": true,
        "id": "yYFH3z_llMln",
        "outputId": "7fd543ec-abe0-441f-8619-69993ab84aa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "('david gallo : das ist bill lange . ich bin dave gallo .\\n', \"david gallo : this is bill lange . i 'm dave gallo .\\n\")\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to yield list of tokens.\n",
        "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
        "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
        "    for data_sample in data_iter:\n",
        "        yield token_transform[language](data_sample[language_index[language]])\n",
        "\n",
        "# Define special symbols and indices.\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "\n",
        "# Make sure the tokens are in order of their indices to properly insert them in vocab.\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    # Create torchtext's Vocab object.\n",
        "    vocab_transform[ln] = build_vocab_from_iterator(\n",
        "        yield_tokens(train_dataset, ln),\n",
        "        min_freq=1,\n",
        "        specials=special_symbols,\n",
        "        special_first=True,\n",
        "    )\n",
        "\n",
        "# Set ``UNK_IDX`` as the default index. This index is returned when the token is not found.\n",
        "# If not set, it throws ``RuntimeError`` when the queried token is not found in the Vocabulary.\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    vocab_transform[ln].set_default_index(UNK_IDX)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T17:37:25.579931Z",
          "iopub.execute_input": "2024-03-03T17:37:25.580232Z",
          "iopub.status.idle": "2024-03-03T17:37:28.668471Z",
          "shell.execute_reply.started": "2024-03-03T17:37:25.580206Z",
          "shell.execute_reply": "2024-03-03T17:37:28.667483Z"
        },
        "trusted": true,
        "id": "12SVf0_8lMlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function to club together sequential operations\n",
        "def sequential_transforms(*transforms):\n",
        "    def func(txt_input):\n",
        "        for transform in transforms:\n",
        "            txt_input = transform(txt_input)\n",
        "        return txt_input\n",
        "    return func\n",
        "\n",
        "# function to add BOS/EOS and create tensor for input sequence indices\n",
        "def tensor_transform(token_ids: List[int]):\n",
        "    return torch.cat((torch.tensor([BOS_IDX]),\n",
        "                      torch.tensor(token_ids),\n",
        "                      torch.tensor([EOS_IDX])))\n",
        "\n",
        "# `src` and `tgt` language text transforms to convert raw strings into tensors indices\n",
        "text_transform = {}\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    text_transform[ln] = sequential_transforms(token_transform[ln], # Tokenization\n",
        "                                               vocab_transform[ln], # Numericalization\n",
        "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
        "\n",
        "\n",
        "# function to collate data samples into batch tensors\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
        "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=True)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX, batch_first=True)\n",
        "    return src_batch, tgt_batch"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T17:37:28.669871Z",
          "iopub.execute_input": "2024-03-03T17:37:28.670627Z",
          "iopub.status.idle": "2024-03-03T17:37:28.679875Z",
          "shell.execute_reply.started": "2024-03-03T17:37:28.670590Z",
          "shell.execute_reply": "2024-03-03T17:37:28.678994Z"
        },
        "trusted": true,
        "id": "vWstkBQrlMlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
        "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
        "EMB_SIZE = 256\n",
        "NHEAD = 8\n",
        "FFN_HID_DIM = 192\n",
        "BATCH_SIZE = 64\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "NUM_DECODER_LAYERS = 3\n",
        "DEVICE = 'cuda'\n",
        "NUM_EPOCHS = 5"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T17:37:28.681244Z",
          "iopub.execute_input": "2024-03-03T17:37:28.681592Z",
          "iopub.status.idle": "2024-03-03T17:37:28.696392Z",
          "shell.execute_reply.started": "2024-03-03T17:37:28.681561Z",
          "shell.execute_reply": "2024-03-03T17:37:28.695570Z"
        },
        "trusted": true,
        "id": "OA_Efk_2lMlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "    src_seq_len = src.shape[1]\n",
        "    tgt_seq_len = tgt.shape[1]\n",
        "\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
        "\n",
        "    src_padding_mask = (src == PAD_IDX)\n",
        "    tgt_padding_mask = (tgt == PAD_IDX)\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T17:37:28.697791Z",
          "iopub.execute_input": "2024-03-03T17:37:28.698402Z",
          "iopub.status.idle": "2024-03-03T17:37:28.710026Z",
          "shell.execute_reply.started": "2024-03-03T17:37:28.698369Z",
          "shell.execute_reply": "2024-03-03T17:37:28.709149Z"
        },
        "trusted": true,
        "id": "FnEccw0ZlMlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout, max_len=5000):\n",
        "        \"\"\"\n",
        "        :param max_len: Input length sequence.\n",
        "        :param d_model: Embedding dimension.\n",
        "        :param dropout: Dropout value (default=0.1)\n",
        "        \"\"\"\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Inputs of forward function\n",
        "        :param x: the sequence fed to the positional encoder model (required).\n",
        "        Shape:\n",
        "            x: [sequence length, batch size, embed dim]\n",
        "            output: [sequence length, batch size, embed dim]\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T17:37:28.712706Z",
          "iopub.execute_input": "2024-03-03T17:37:28.713000Z",
          "iopub.status.idle": "2024-03-03T17:37:28.728518Z",
          "shell.execute_reply.started": "2024-03-03T17:37:28.712976Z",
          "shell.execute_reply": "2024-03-03T17:37:28.727722Z"
        },
        "trusted": true,
        "id": "cu7IsuUSlMlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T17:37:28.729730Z",
          "iopub.execute_input": "2024-03-03T17:37:28.730524Z",
          "iopub.status.idle": "2024-03-03T17:37:28.743102Z",
          "shell.execute_reply.started": "2024-03-03T17:37:28.730492Z",
          "shell.execute_reply": "2024-03-03T17:37:28.742105Z"
        },
        "trusted": true,
        "id": "DdGhLiIBlMlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_encoder_layers: int,\n",
        "        num_decoder_layers: int,\n",
        "        emb_size: int,\n",
        "        nhead: int,\n",
        "        src_vocab_size: int,\n",
        "        tgt_vocab_size: int,\n",
        "        dim_feedforward: int = 512,\n",
        "        dropout: float = 0.1\n",
        "    ):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        self.transformer = Transformer(\n",
        "            d_model=emb_size,\n",
        "            nhead=nhead,\n",
        "            num_encoder_layers=num_encoder_layers,\n",
        "            num_decoder_layers=num_decoder_layers,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            emb_size, dropout=dropout)\n",
        "    def forward(self,\n",
        "                src: Tensor,\n",
        "                trg: Tensor,\n",
        "                src_mask: Tensor,\n",
        "                tgt_mask: Tensor,\n",
        "                src_padding_mask: Tensor,\n",
        "                tgt_padding_mask: Tensor,\n",
        "                memory_key_padding_mask: Tensor):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
        "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
        "        return self.generator(outs)\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        return self.transformer.encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer.decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T17:37:28.744309Z",
          "iopub.execute_input": "2024-03-03T17:37:28.744556Z",
          "iopub.status.idle": "2024-03-03T17:37:28.760436Z",
          "shell.execute_reply.started": "2024-03-03T17:37:28.744535Z",
          "shell.execute_reply": "2024-03-03T17:37:28.759563Z"
        },
        "trusted": true,
        "id": "0JIulFDtlMlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = Seq2SeqTransformer(\n",
        "    NUM_ENCODER_LAYERS,\n",
        "    NUM_DECODER_LAYERS,\n",
        "    EMB_SIZE,\n",
        "    NHEAD,\n",
        "    SRC_VOCAB_SIZE,\n",
        "    TGT_VOCAB_SIZE,\n",
        "    FFN_HID_DIM\n",
        ").to(DEVICE)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T17:37:45.195365Z",
          "iopub.execute_input": "2024-03-03T17:37:45.196276Z",
          "iopub.status.idle": "2024-03-03T17:37:45.998062Z",
          "shell.execute_reply.started": "2024-03-03T17:37:45.196233Z",
          "shell.execute_reply": "2024-03-03T17:37:45.996945Z"
        },
        "trusted": true,
        "id": "NhW3R6yYlMlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T17:37:46.001170Z",
          "iopub.execute_input": "2024-03-03T17:37:46.001563Z",
          "iopub.status.idle": "2024-03-03T17:37:49.244344Z",
          "shell.execute_reply.started": "2024-03-03T17:37:46.001529Z",
          "shell.execute_reply": "2024-03-03T17:37:49.243497Z"
        },
        "trusted": true,
        "id": "-zgxBSb7lMlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "def train_epoch(model, optimizer):\n",
        "    print('Training')\n",
        "    model.train()\n",
        "    losses = 0\n",
        "\n",
        "    for src, tgt in tqdm(train_dataloader, total=len(list(train_dataloader))):\n",
        "        # print(\" \".join(vocab_transform[SRC_LANGUAGE].lookup_tokens(list(src[0].cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\"))\n",
        "        # print(\" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt[0].cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\"))\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:, :-1]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(\n",
        "            src,\n",
        "            tgt_input,\n",
        "            src_mask,\n",
        "            tgt_mask,\n",
        "            src_padding_mask,\n",
        "            tgt_padding_mask,\n",
        "            src_padding_mask\n",
        "        )\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        tgt_out = tgt[:, 1:]\n",
        "        loss = loss_fn(logits.view(-1, TGT_VOCAB_SIZE), tgt_out.contiguous().view(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(list(train_dataloader))\n",
        "\n",
        "\n",
        "val_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "def evaluate(model):\n",
        "    print('Validating')\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "\n",
        "    for src, tgt in tqdm(val_dataloader, total=len(list(val_dataloader))):\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:, :-1]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(\n",
        "            src,\n",
        "            tgt_input,\n",
        "            src_mask,\n",
        "            tgt_mask,\n",
        "            src_padding_mask,\n",
        "            tgt_padding_mask,\n",
        "            src_padding_mask\n",
        "        )\n",
        "\n",
        "        tgt_out = tgt[:, 1:]\n",
        "        loss = loss_fn(logits.view(-1, TGT_VOCAB_SIZE), tgt_out.contiguous().view(-1))\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(list(val_dataloader))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T17:37:51.006371Z",
          "iopub.execute_input": "2024-03-03T17:37:51.006968Z",
          "iopub.status.idle": "2024-03-03T17:37:51.023656Z",
          "shell.execute_reply.started": "2024-03-03T17:37:51.006933Z",
          "shell.execute_reply": "2024-03-03T17:37:51.022004Z"
        },
        "trusted": true,
        "id": "SbXOuwsNlMlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss_list, valid_loss_list = [], []\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    start_time = timer()\n",
        "    train_loss = train_epoch(transformer, optimizer)\n",
        "    valid_loss = evaluate(transformer)\n",
        "    end_time = timer()\n",
        "    train_loss_list.append(train_loss)\n",
        "    valid_loss_list.append(valid_loss)\n",
        "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {valid_loss}\"))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T17:37:51.609969Z",
          "iopub.execute_input": "2024-03-03T17:37:51.610731Z"
        },
        "trusted": true,
        "id": "eFrzdx05lMlq",
        "outputId": "abf2a2be-89c8-4dee-fa22-39695f6e1f9e",
        "colab": {
          "referenced_widgets": [
            "189ca95903ba42c69cd74334e7e57711"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Training\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/3062 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "189ca95903ba42c69cd74334e7e57711"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n  warnings.warn(\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(DEVICE)\n",
        "        if i == 0:\n",
        "            ys = ys.transpose(1, 0)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(1))\n",
        "                    .type(torch.bool)).to(DEVICE)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.item()\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "# Translation function.\n",
        "def translate(model: torch.nn.Module, src_sentence: str):\n",
        "    model.eval()\n",
        "    src = text_transform[SRC_LANGUAGE](src_sentence).view(1, -1)\n",
        "    num_tokens = src.shape[1]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = greedy_decode(\n",
        "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
        "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "wq_Vnt3GlMlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/kaggle/input/bdz2aa/test1.de-en.de\", \"r\", encoding=\"utf-8\") as file_input:\n",
        "    with open(\"solution.txt\", \"w\", encoding=\"utf-8\") as file_output:\n",
        "        for sentence in file_input.readlines():\n",
        "            print(translate(transformer, sentence).strip(\" \"), file=file_output)"
      ],
      "metadata": {
        "trusted": true,
        "id": "9m086K3TlMlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dgov_8UslMlq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}